[
{
	"uri": "/ServicesAnalytics/intro/",
	"title": "소개",
	"tags": [],
	"description": "",
	"content": "이 워크샵을 통해 AWS 기반의 데이터 분석을 실습합니다.준비사항  AWS 계정: EC2, S3, IAM, Kinesis(Data Analytics, Data Firehose), Glue, Athena, QuickSight, Redshift 자원을 생성할 수 있는 권한이 필요합니다. AWS 리전: 이번 실습은 오레곤 (us-west-2) 리전에서 실행합니다. 브라우저: 최신 버전의 크롬, 파이어폭스를 사용하세요.  Contact Us AWS 서비스에 관한 질문은 AWS Support나 담당 AM을 통해서 문의해 주시고 본 워크샵의 발표자료에 관한 질문 사항은 아래의 email 링크를 통해 문의해 주시면 감사하겠습니다. "
},
{
	"uri": "/ServicesAnalytics/",
	"title": "홈",
	"tags": [],
	"description": "",
	"content": "AWS Analytics 서비스 워크샵   "
},
{
	"uri": "/ServicesAnalytics/lab0/",
	"title": "실습0. 사전 준비",
	"tags": [],
	"description": "",
	"content": "본격적인 Lab 시작에 앞서 구성에 필요한 IAM User, EC2, S3를 생성 및 구성합니다.IAM User 생성 Lab 전체에서 사용할 IAM User를 생성합니다.  AWS Management Console에 로그인 한 뒤 IAM 서비스에 접속합니다.\n  왼쪽 메뉴에서 Users를 선택합니다.   Add user 버튼을 클릭하여 사용자 추가 페이지로 들어갑니다.\n  User name에 \u0026lt;원하는 사용자 이름\u0026gt; 을 입력하고, Access type에 Programmatic access와 AWS Management Console access 둘 모두를 선택합니다. Console password에 \u0026lt;원하는 패스워드\u0026gt;를 입력하고, 마지막 Require password reset의 체크는 해제합니다.   Next: Permissions 버튼을 클릭하고 Attach existing policies directly를 선택한 뒤 AdministratorAccess 권한을 추가해줍니다.   Next: Review 버튼을 클릭하고 정보를 확인한 뒤 Create user 버튼을 클릭하여 사용자 생성을 완료합니다.   Download.csv 버튼을 클릭하여 생성한 사용자의 정보를 다운 받습니다. EC2 설정에 꼭 필요한 파일이므로 기억하기 쉬운 위치에 저장합니다.   AWS Management Console에 로그인 한 뒤 IAM 서비스에 접속합니다.\n  왼쪽 메뉴에서 Roles를 선택합니다. 상단의 Create role 버튼 클릭하여 role 생성 page로 들어갑니다.   AWS service 선택, EC2 service 를 선택 합니다. Next: Permissions를 선택합니다.   Filter에 \u0026ldquo;SSM\u0026rdquo; 을 입력하고, AmazonEC2RoleforSSM를 선택합니다. Next:Tags 버튼을 클릭해서 tags 입력 부분으로 들어가서, Next:Review 버튼을 클릭합니다.   Role name에는 EC2RoleforSSM을 입력하고, Create role 버튼을 클릭합니다.   EC2 생성 Lab에서 데이터를 실시간으로 발생시킬 EC2 인스턴스를 생성합니다. AWS Management Console에서 EC2 서비스에 접속합니다. 우측 상단에서 Region은 **US West (Oregon)**를 선택합니다. Launch Instance를 선택하여 새로운 인스턴스 생성을 시작합니다.  Step 1: Choose an Amazon Machine Image (AMI) 화면에서 Amazon Linux AMI 2018.03.0 (HVM), SSD Volume Type - ami-14c5486b 를 선택합니다.  Step 2 : Choose an Instance Type 화면에서 인스턴스 타입은 t2.micro를 선택합니다. Next: Configure Instance Details 을 클릭합니다.  Step 3: Configure Instance Details 화면에서 IAM role에서 미리 생성한 EC2RoleforSSM을 선택 합니다.  Step 3: Configure Instance Details 화면에서 Advanced Details을 클릭하고 아래 userdata를 복사하여 붙여 넣습니다.  #include\rhttps://s3.amazonaws.com/immersionday-bigdata-v20180731/userdata.sh\r8. Step 4: Add Storage 화면에서 기본값을 그대로 두고 Next: Add Tags를 클릭합니다. 9. Step 5: Add Tags 화면에서 Add Tag 버튼을 한 번 클릭한 뒤, Key/Value : Name/AnalyticsStream 를 입력하고 Next: Configure Security Group 을 클릭합니다. 10. Step 6: Configure Security Group 화면에서 Security Group에 필요한 정보를 입력한 후 Review and Launch를 클릭합니다. * Security Group Name : bastion * Description : SG for bastion * Type : SSH * Protocol : TCP * Port Range : 22 * Source : 0.0.0.0/0 11. Step 7: Review Instance Launch 화면에서 Launch를 클릭합니다. 12. EC2 Instance에 접속하기 위한 Key pair를 생성합니다. Create a new key pair를 선택하고 Key pair name은 analytics-hol 을 입력한 후 Download Key Pair를 클릭합니다. 13. Key Pair를 PC의 임의 위치에 저장한 후 Launch Instances를 클릭합니다. (인스턴스 기동에 몇 분이 소요될 수 있습니다.) 14. (MacOS 사용자) 다운로드 받은 Key Pair 파일의 File Permission을 400으로 변경합니다.\n$ chmod 400 ./analytics-hol.pem\r$ ls -lat analytics-hol.pem\r-r-------- 1 ****** ****** 1692 Jun 25 11:49 analytics-hol.pem\rEC2 설정 - System manager (seesion manager를 통해 ec2 ssh 접속) 생성한 EC2 인스턴스가 다른 AWS 리소스에 접근 및 제어할 수 있도록 다음과 같이 구성합니다. AWS Management Console에서 Systems Manager 서비스에 접속합니다. 왼쪽 하단의 Session Manager 를 선택 합니다.  Session manager console에서 Start session 버튼을 클릭합니다.  앞에서 생성한 AnalyticsStream instance를 선택하고, 하단의 Start session 버튼을 클릭합니다.  instance session에 들어가서 /home/ec2-user/ 에 있는 3개의 files을 /home/ssm-user/ 로 copy 합니다.  sudo cp /home/ec2-user/redshift.py /home/ssm-user/\rsudo cp /home/ec2-user/firehose.py /home/ssm-user/\rsudo cp /home/ec2-user/banking_loss.csv /home/ssm-user/\rUser Data를 통해 3가지 파일(banking_loss.csv, firehose.py, redshift.py)이 잘 다운로드 받아졌는지 확인합니다.  ls /home/ssm-user/\rAWS의 다른 리소스 접근을 위해 AWS Configure를 진행합니다. 이때 앞서 생성한 IAM User 데이터를 활용합니다. 이전에 다운로드 받은 .csv 파일을 열어 Access key ID와 Secret access key를 확인하고 입력합니다. region은 us-west-2 로 입력 합니다.  aws configure\rAWS Access Key ID [None]: \u0026lt;Access key ID\u0026gt;\rAWS Secret Access Key [None]: \u0026lt;Secret access key\u0026gt;\rDefault region name [None]: us-west-2\rDefault output format [None]:\r설정이 완료 되었다면, aws configure list 로 다음과 같이 입력하신 정보가 마스킹 되어 보이게 됩니다.  aws configure list\rS3 Bucket 생성 발생한 실시간 데이터를 저장할 S3 Bucket을 생성합니다. AWS Management Console에서 S3 서비스에 접속합니다. Create bucket 버튼을 클릭하고 다음과 같은 정보를 입력한 뒤 Create 버튼을 클릭하여 bucket을 생성합니다.             Bucket name analytics-workshop-[개인식별자] (예 : analytics-workshop-foobar)   Region 미국 서부 (오레곤)   그 외 default     "
},
{
	"uri": "/ServicesAnalytics/lab1/",
	"title": "실습1. Kinesis로 S3에 데이터 수집",
	"tags": [],
	"description": "",
	"content": "Lab 설명 이번 Lab은 EC2에서 발생하는 데이터를 Kinesis Firehose를 통해 수집하여 S3에 저장합니다. 그 후 Kinesis Analytics로 실시간 쿼리해 분석해보고, 결과를 S3에 다시 저장합니다.\nLab Architecture Kinesis Firehose 생성 (원본 데이터) Kinesis Firehose를 통해 앞서 생성한 EC2가 발생시키는 실시간 데이터를 S3, Redshift, ElasticSearch 등의 목적지에 수집할 수 있습니다. AWS Management Console에서 Kinesis 서비스를 선택합니다. (Region : Oregon) Get Started 버튼을 클릭합니다. Deliver streaming data with Kinesis Firehose delivery streams 메뉴의 Create delivery stream 을 클릭하여 새로운 Firehose 전송 스트림 생성을 시작합니다.  Delivery stream name에 Source 를 입력한 뒤 Next를 클릭합니다.(주의 : Source 이외의 name을 지정할 경우 EC2에서 Kinesis Firehose로 log가 전송되지 않습니다.) Record transformation / Record format conversion 은 default인 Disabled로 두고 Next를 클릭합니다. Destination은 Amazon S3를 선택하고 아래 S3 bucket은 앞서 생성한 bucket을 선택합니다. Prefix에는 source/ 를 입력합니다. Next를 클릭합니다  Buffer size는 1MB, Buffer interval은 60 seconds로 설정합니다. 아래 IAM role에서 Create new, or Choose 버튼을 클릭합니다.  새로 열린 탭에서 필요한 정책이 포함된 IAM 역할 firehose_delivery_role을 자동으로 생성합니다. Allow 버튼을 클릭하여 진행합니다.  새롭게 생성된 역할이 추가된 것을 확인한 뒤 Next 버튼을 클릭합니다.  Review에서 입력한 정보를 확인한 뒤 틀린 부분이 없다면 Create delivery stream 버튼을 클릭하여 Firehose 생성을 완료합니다.  Firehose delivery streams 가 생성 되었습니다.   데이터 수집 확인 생성한 Firehose가 정상적으로 데이터를 수집하는지 확인해봅니다. 앞서 생성한 EC2 인스턴스에 system manager - session manager로 접속을 합니다. 홈 디렉토리로 이동 후, firehose.py 를 실행합니다.  cd ~\rpython firehose.py\r매 초 데이터가 발생하는 것을 확인합니다. 충분한 데이터 수집을 위해 실행 중인 상태로 다음 단계를 진행합니다. 몇 분 뒤 생성한 S3 bucket에 가보면 생성된 원본 데이터가 Firehose를 통해 S3에 저장되는 것을 확인할 수 있습니다. 이 때 지정한 Prefix인 source 폴더에 데이터가 저장됩니다.   Kinesis Firehose 생성 (아웃풋 데이터) 앞서 생성한 Firehose 전송 스트림과 동일한 방법으로 Destination 전송 스트림을 생성합니다. 이 때, Delivery stream name과 S3 destination의 Prefix만 새로운 값을 입력합니다. Delivery stream name에 Destination 를 입력한 뒤 Next를 클릭합니다.  Destination은 Amazon S3를 선택하고 아래 S3 bucket은 앞서 생성한 bucket을 선택합니다. Prefix에는 destination/ 을 입력합니다. Next를 클릭합니다.  Buffer size는 1MB, Buffer interval은 60 seconds로 설정합니다. IAM Role은 앞서 생성된 firehose_delivery_role을 선택합니다. Policy Name은 Create a new Role Policy를 선택합니다.  Source, Destination 2개의 Firehose 전송 스트림 생성을 완료합니다.   Kinesis Analytics Application - 생성 Kinesis Analytics를 활용하여 스트리밍 데이터를 처리할 application을 생성합니다. AWS Management Console에 로그인 한 뒤 Kinesis 서비스에 접속합니다. Kinesis analytics applications 의 Create analytics application을 선택합니다.  Application name에 StreamApplication을 입력하고 Create application 버튼을 클릭합니다.   Kinesis Analytics Application - Source 지정  생성한 application 화면으로 이동 후 Connect streaming data 버튼을 클릭합니다.  Source는 Kinesis Firehose delivery stream 을 선택하고 앞서 생성한 2개의 스트림 중 Source 를 선택합니다. [Discover Schema] 버튼을 클릭하여 Schema를 분석합니다. Schema 분석 후 아래에 샘플 데이터를 보여줍니다. (Schema 탐색이 실패했다면 EC2 인스턴스에서 firehose.py 가 실행 중 인지 확인합니다)  Schema분석이 끝나면 DATE 데이터 타입을 지정해주기 위해 Edit schema 버튼을 클릭합니다.  OccurrenceStartDate와 DiscoveryDate의 타입을 DATE로 변경해줍니다. NetLoss, RecoveryAmount, EstimatedGrossLoss는 DOUBLE로 변경해줍니다. Save schema and update stream samples 버튼을 클릭하여 저장합니다.  저장이 완료되면 Exit(done) 버튼을 클릭하여 Source 지정을 완료합니다.  Kinesis Analytics Application - SQL 실시간 쿼리  Go to SQL editor 메뉴로 이동합니다.  Text editor에 다음과 같은 SQL문을 입력한 뒤 Save and run SQL 버튼을 클릭합니다. (이 때 Source data가 없다는 문구가 표시된다면 EC2에서 firehose.py가 실행 중인지 확인 후 Refresh 합니다.)  CREATE OR REPLACE STREAM \u0026quot;DESTINATION_SQL_BASIC_STREAM\u0026quot; (\rRegion VARCHAR(16), Business VARCHAR(32), Name VARCHAR(16), Status VARCHAR(16), RiskCategory VARCHAR(64), RiskSubCategory VARCHAR(64),\rDiscoveryDate DATE, OccurrenceStartDate DATE, NetLoss DOUBLE,\rRecoveryAmount DOUBLE, EstimatedGrossLoss DOUBLE);\rCREATE OR REPLACE PUMP \u0026quot;STREAM_PUMP_1\u0026quot; AS INSERT INTO\r\u0026quot;DESTINATION_SQL_BASIC_STREAM\u0026quot;\rSELECT STREAM \u0026quot;Region\u0026quot;, \u0026quot;Business\u0026quot;, \u0026quot;Name\u0026quot;, \u0026quot;Status\u0026quot;, \u0026quot;RiskCategory\u0026quot;, \u0026quot;RiskSubCategory\u0026quot;, \u0026quot;DiscoveryDate\u0026quot;, \u0026quot;OccurrenceStartDate\u0026quot;, \u0026quot;NetLoss\u0026quot;, \u0026quot;RecoveryAmount\u0026quot;, \u0026quot;EstimatedGrossLoss\u0026quot; FROM \u0026quot;SOURCE_SQL_STREAM_001\u0026quot;;\rSQL 쿼리가 실시간으로 수행되는 것을 확인할 수 있습니다. Close 버튼을 클릭합니다.   Kinesis Analytics Application - Destination 지정  Kinesis Analytics의 SQL 수행 결과를 S3에 저장할 수 있습니다. Kinesis Analytics StreamApplication 화면으로 돌아가 Connect to a destination 을 클릭합니다.    Destination : Kinesis Firehose delivery stream Kinesis Firehose delivery stream : Destination In-application stream name : DESTINATION_SQL_BASIC_STREAM, Output format : JSON Access to chosen resources : Create update IAM role kinesis-analytics-StreamApplication-us-west-2 을 지정한 후 Save and Continue 버튼을 클릭합니다.    Destination이 설정 되었습니다. Exit to Kinesis Analytics applications 을 클릭합니다.  EC2에서 firehose.py 실행을 중단했다면 다시 실행합니다. 몇 분 뒤 S3 bucket을 보면 설정한 Prefix인 destination 폴더가 생성된 것을 확인할 수 있습니다. 해당 폴더에는 설정한대로 JSON 포맷으로 데이터가 수집됩니다.    "
},
{
	"uri": "/ServicesAnalytics/lab2/",
	"title": "실습2. API Gateway, Lambda로 Aurora 데이터 가져오기",
	"tags": [],
	"description": "",
	"content": "실습 소개 이번 실습 에서는 Aurora에 저장하고 API Gateway, Lambda를 이용해서 Aurora에 저장된 데이터를 가져오는 실습을 진행할 예정입니다.\n\r 실습 아키텍처  CloudFormation으로 Aurora Serverless 구성 CloudFormation은 사용하면 텍스트 파일 또는 프로그래밍 언어로 전체 인프라와 애플리케이션 리소스를 모델링할 수 있습니다. 이를 통하여 VPC 및 Aurora Serverless몇 번의 클릭 만으로 구성 할 수 있습니다.\n AWS Management Console에 로그인 합니다. (Region : us-east-1) AWS Cloudformation for Aurora Serverless 접속합니다. 기본 설정을 확인 합니다. VPC 및 Subnet CIDR이 기존에 생성되어 있는 것과 겹치지 않으면, 변경 하지 않고, 화면 제일 하단에 있는 체크 박스 두개를 모두 선택한 후 스택 생성\r\r버튼을 클릭합니다.  \r스택 생성\r\r버튼을 누르면 아래와 같은 화면으로 리다이렉트 됩니다. 모든 Stack이 CREATE_COMPLETE 될때 까지 기다립니다.(약 10분 ~ 20분 소요)    Query Editor를 이용하여 간단한 쿼리 수행 CloudFormation으로 Aurora Serverless를 프로비져닝 하였습니다. Query Editor를 이용하여 Sample Query를 수행합니다. 그리고 집계 테이블을 가져오기 위한 Glue ETL 작업을 수행하기 전에 Aurora에 테이블을 생성 합니다.\n RDS Console에 접속합니다. 좌측 메뉴에서 Query Editor를 선택합니다.  생성된 Aurora Cluster my-aurora-cluster 를 선택하고, 데이터베이스 사용자 이름은 새 데이터베이스 자격 증명 추가를 선택 합니다.  옵션을 아래와 같이 입력 합니다.\r데이터베이스에 연결\r\r버튼을 누릅니다.  데이터베이스 사용자 이름 입력: MyAdmin 데이터베이스 암호 입력: Init12345 데이터베이스 이름 입력: mydb    연결 중\u0026hellip; 에서 편집기로 리다이렉트 됩니다. 생성 된 Sample Query를 수행 하기 위해,\r실행\r\r버튼을 누릅니다. Aurora Serverless 이기 때문에 약간 delay가 발생할 수 있습니다.  select * from information_schema.tables;\r7. 테스트 하기 위한 간단한 테이블을 생성합니다. 아래 쿼리를 수행 후 실행\r\r버튼을 누릅니다. 실행 후 Status가 성공 인지 확인 합니다.\nCREATE TABLE user_table(name VARCHAR(255), role VARCHAR(255));\r AWS Lambda 생성 Aurora에 저장된 데이터를 가지고 올 수 있는 AWS Lambda 함수를 생성합니다.\n Lambda를 생성하기 전에 Aurora Serverless를 접속하기 위한 접속 정보를 확인 합니다.  cluster arn 확인 하는 방법  Aurora 데이터베이스 Console에 접속합니다. 생성된 my-aurora-cluster 클릭 합니다.  구성 탭을 선택하고 cluster arn 정보를 확인 합니다.(e.g arn:aws:rds:us-east-1:773728249266:cluster:my-aurora-cluster)    secret arn 확인 하는 방법  Secrets Manager Console에 접속합니다. Aurora 접속시 생성된 보안 암호 이름 을 선택합니다  보안 암호 ARN을 확인 합니다 (e.g arn:aws:secretsmanager:us-east-1:773728249266:secret:rds-db-credentials/cluster-NE5IADIOTWINCMB4JMZMV3KVDQ/MyAdmin-S7nkrf)      Lambda Console에 접속합니다. \r함수 생성\r\r버튼을 누릅니다.  새로 작성 상태에서, 함수 이름은 MyAurora, 런타임은 Python 3.7 을 선택하고, 함수 생성\r\r버튼을 누릅니다.  함수 코드를 아래 코드로 변경 합니다. cluster_arn 과 secret_arn은 확인한 정보로 변경 합니다. 변경 후 저장\r\r버튼을 누릅니다.  import boto3 import json # Update your cluster and secret ARNs cluster_arn = \u0026#39;\u0026lt;arn:aws:rds:us-east-1:~\u0026gt;\u0026#39; secret_arn = \u0026#39;\u0026lt;arn:aws:secretsmanager:~\u0026gt;\u0026#39; def lambda_handler(event, context): operation = event[\u0026#39;operation\u0026#39;] if \u0026#39;get\u0026#39; == operation: queryResult = get_rds_data_api() elif \u0026#39;para\u0026#39; == operation: queryResult = get_para_rds_data_api(event[\u0026#39;payload\u0026#39;][\u0026#39;role\u0026#39;]) return queryResult def get_rds_data_api(): rds_data = boto3.client(\u0026#39;rds-data\u0026#39;) sql = \u0026#34;\u0026#34;\u0026#34; SELECT * FROM user_table \u0026#34;\u0026#34;\u0026#34; response = rds_data.execute_statement( resourceArn = cluster_arn, secretArn = secret_arn, database = \u0026#39;mydb\u0026#39;, sql = sql) return response def get_para_rds_data_api(payload): rds_data = boto3.client(\u0026#39;rds-data\u0026#39;) sql = \u0026#34;\u0026#34;\u0026#34; SELECT * FROM user_table where role = :para1 \u0026#34;\u0026#34;\u0026#34; message_value = payload para1 = [{\u0026#39;name\u0026#39;:\u0026#39;para1\u0026#39;, \u0026#39;value\u0026#39;:{\u0026#39;stringValue\u0026#39;: f\u0026#39;{message_value}\u0026#39;}}] response = rds_data.execute_statement( resourceArn = cluster_arn, secretArn = secret_arn, database = \u0026#39;mydb\u0026#39;, sql = sql, parameters = para1) return response Lambda가 Aurora를 접속 할 수 있도록, 실행 권한을 부여합니다. 우측 상단에 권한 탭을 선택 후, 역할 이름 아래 있는 링크를 클릭 합니다.  IAM 콘솔로 리다이렉트 됩니다. 정책 연결\r\r버튼을 누릅니다.  AmazonRDSDataFullAccess 정책을 검색한 후 정책 연결\r\r버튼을 누릅니다.    "
},
{
	"uri": "/ServicesAnalytics/lab3/",
	"title": "실습3. EMR을 이용한 Hadoop 클러스터 구성 및 데이터 분석",
	"tags": [],
	"description": "",
	"content": "Amazon EMR은 관리형 Hadoop 프레임워크로서 빠르게 빅데이터 분석을 위한 Hadoop 클러스터 구성을 할 수 있습니다.EMR 클러스터 생성 EMR 클러스터를 구성합니다. 본 실습에서는 Spark와 Zeppelin을 활용합니다. AWS 관리 콘솔에 로그인 한 뒤, Oregon 리전의 EMR 서비스에 접속합니다. \r클러스터 생성\r\r버튼을 선택하여 클러스터 구성을 시작합니다. 고급 옵션으로 이동 버튼을 선택하여 원하는 애플리케이션을 직접 선택하여 구성합니다. Release는 emr-5.27.0, 소프트웨어는 Spark 2.4.4과 Zeppelin 0.8.1를 선택하고 \u0026ldquo;Hive 테이블 메타 데이터에서 사용\u0026rdquo; 및 **\u0026ldquo;Spark 테이블 메타 데이터에서 사용\u0026rdquo;**을 선택합니다.  하드웨어 구성 단계에서는 네트워크 선택에서 vpc-xxxxx 를 선택하고 EC2 서브넷에서는 가장 상단의 항목을 선택합니다. 나머지 항목은 그대로 두고 다음\r\r을 클릭합니다.  클러스터 이름에 원하는 클러스터 이름을 입력하고, 종료 보호 옵션은 체크 해제 합니다. 다음\r\r을 클릭합니다.  보안 옵션에서 앞에서 생성한 EC2 키 페어를 선택합니다.  다음과 같이 두 단계로 보안을 설정합니다. 1 단계) EC2 보안 그룹섹션에서 마스터와 코어 및 작업의 EMR 관리형 보안 그룹에는 모두 각각 default 그룹을 설정하고 2 단계) 마스터 노드의 추가 보안 그룹에는 사전 준비에서 생성했던 보안 그룹 (포트 22 허용)을 추가로 할당합니다.  화면 하단의 클러스터 생성을 선택하여 클러스터를 생성합니다.EMR 클러스터 생성에 약 10분~15분 정도 소요됩니다.  클러스터의 상태가 **대기 (클러스터 준비)**이 되면 다음 실습을 진행합니다.   EMR 웹 접속 설정 (윈도 운영체제)  생성한 클러스터를 선택하여 마스터 퍼블릭 DNS 정보를 확인합니다.  SSH의 Tunnels에서 Source port에 8157을 Destination에는 localhost:8890 을 입력한 뒤 Add를 클릭합니다.  Auth에는 EMR 클러스터 생성시에 사용한 키 페어의 .ppk 키를 추가합니다. 마지막으로 Session의 Host Name에는 다음을 입력한 뒤 Open을 클릭하여 접속합니다.  hadoop@\u0026lt;마스터 퍼블릭 DNS\u0026gt;\r브라우저에서 http://localhost:8157 주소로 접속합니다.   EMR 웹 접속 설정 (Mac 또는 Linux 운영체제)  간단히 마스터 노드와의 SSH 터널을 만들어 접근할 수 있습니다. 마스터 퍼블릭 DNS를 확인합니다.  터미널에서 다음 명령어를 입력합니다. 응답을 반환 하지는 않습니다.  ssh -i \u0026quot;\u0026lt;키 페어 이름\u0026gt;\u0026quot; -N -L 8157:localhost:8890 hadoop@\u0026lt;마스터 퍼블릭 DNS\u0026gt;\r브라우저에서 http://localhost:8157 주소로 접속합니다.   Zeppelin을 활용한 빅데이터 분석  Zeppelin Tutorial - Basic Features (Spark)를 클릭합니다.  여러분은 S3 (EMRFS)에 저장된 데이터(각 필드가 ‘;’로 구분)로 부터 6개의 컬럼을 읽어 필요한 자료형을 적용하고 이를 임시 테이블로 저장하는 코드를 볼 수 있습니다.  원본 코드에서 추가로 몇몇 컬럼을 더 가져오기(6번째 12번째 컬럼을 추가로 활용) 위하여 https://github.com/setch3000/emr-s3/blob/master/sample.scala 에서 샘플 코드를 복사하여 Zepplelin 노트북의 셀에 붙여 넣기를 한 후 ▷버튼을 눌려 다시 한번 실행 해 봅니다.   %spark\rimport org.apache.commons.io.IOUtils\rimport java.net.URL\rimport java.nio.charset.Charset\r//Load Bank Data\rval bankText = sc.parallelize(\rIOUtils.toString(\rnew URL(\u0026quot;https://s3.amazonaws.com/apache-zeppelin/tutorial/bank/bank.csv\u0026quot;),\rCharset.forName(\u0026quot;utf8\u0026quot;)).split(\u0026quot;\\n\u0026quot;))\rcase class Order(age: Integer, job: String, marital: String, education: String, amount: Integer, housing: String, campaign: String)\rval bank = bankText.map(s =\u0026gt; s.split(\u0026quot;;\u0026quot;)).filter(s =\u0026gt; s(0) != \u0026quot;\\\u0026quot;age\\\u0026quot;\u0026quot;).map(\rs =\u0026gt; Order(s(0).toInt,\rs(1).replaceAll(\u0026quot;\\\u0026quot;\u0026quot;, \u0026quot;\u0026quot;),\rs(2).replaceAll(\u0026quot;\\\u0026quot;\u0026quot;, \u0026quot;\u0026quot;),\rs(3).replaceAll(\u0026quot;\\\u0026quot;\u0026quot;, \u0026quot;\u0026quot;),\rs(5).replaceAll(\u0026quot;\\\u0026quot;\u0026quot;, \u0026quot;\u0026quot;).toInt,\rs(6).replaceAll(\u0026quot;\\\u0026quot;\u0026quot;, \u0026quot;\u0026quot;),\rs(12).replaceAll(\u0026quot;\\\u0026quot;\u0026quot;, \u0026quot;\u0026quot;)\r)\r).toDF()\rbank.registerTempTable(\u0026quot;bank\u0026quot;)\rbank.show()\r로딩 된 데이터를 바탕으로 다양한 분석을 진행 해 봅니다.4.1 결혼 여부 / 주택 소유 여부에 따른 평균 이용 금액  %sql\rselect marital,housing,\ravg(amount)\rfrom bank\rgroup by marital, housing\r4.2 직업별 평균 이용 금액\n%sql\rselect job, avg(amount)\rfrom bank\rgroup by job\rorder by 2\r4.3 20대 나이별 평균 이용 금액\n%sql\rselect age, avg(amount)\rfrom bank\rwhere age \u0026gt; ${minAge=19} and\rage \u0026lt; ${maxAge=30}\rgroup by age\r4.4 연령/교육 수준별 이용 금액\n%sql\rselect age, amount, education,\rhousing\rfrom bank\r4.5 직업 별 평균 마케팅 노출 횟수\n%sql\rselect job, avg(campaign) as\rcampaign\rfrom bank\rgroup by job\r4.6 결혼 직업 별 이용 금액 합계\n%sql\rselect marital, job, sum(amount)\rfrom bank\rgroup by marital, job\r5. 분석 결과를 그래프로 표현 하면 아래와 같이 원하는 정보를 같은 방식으로 분석할 수 있습니다. Glue 데이터 카탈로그를 활용하여 S3에 저장된 데이터 분석 EMR 클러스터에서 Glue의 카탈로그를 조회하여 손쉽게 S3에 저장된 데이터를 분석 할 수 있습니다. 화면 좌측 상단의 Zeppelin 이미지를 클릭하여 Zeppelin의 첫페이지로 이동합니다. Create new note 버튼을 클릭 합니다.  ’TEST’라는 이름으로 새 노트를 생성 합니다.  Glue의 카탈로그를 조회하여 데이터베이스 리스트를 확인 합니다.  %spark\rspark.sql(\u0026quot;show databases\u0026quot;).show()\r5. 확인된 데이터베이스 리스트 중 \u0026ldquo;workshop\u0026quot;의 테이블들을 확인해 봅니다.\n%spark\rspark.catalog.setCurrentDatabase(\u0026quot;workshop\u0026quot;)\rspark.sql(\u0026quot;show tables\u0026quot;).show()\r6. 다음과 같이 쿼리를 수행하여 데이터 분석을 진행 합니다.\n%sql\rselect region, status, count(*)\rfrom parquet\rgroup by region, status\rorder by region\r선택 작업 : EMR 클러스터에 노드 추가 EMR 클러스터에서 Hadoop의 슬레이브 노드에 해당하는 작업을 실행하는 Core 노드와 Task 노드를 쉽게 추가할 수 있습니다. AWS 관리 콘솔에 로그인 한 뒤, EMR 서비스에 접속합니다. 생성한 클러스터를 선택한 후, 하드웨어 탭을 선택합니다.  CORE의 인스턴스 수의 크기 조정 을 클릭하여 3 으로 변경합니다.  작업 인스턴스 그룹 추가를 클릭하여 작업 노드를 1개 추가합니다.  Core와 Task 노드의 크기 변경이 시작됩니다. 각 노드에 Auto Scaling을 적용할 수도 있습니다.    "
},
{
	"uri": "/ServicesAnalytics/lab4/",
	"title": "실습4. AWS 기반 데이터 웨어하우징 - Amazon Redshift",
	"tags": [],
	"description": "",
	"content": "Redshift 을 위한 IAM Role 설정  AWS Management Console에 로그인 한 뒤 IAM 서비스에 접속합니다. Roles를 클릭하고 Create new role을 클릭합니다. Select type of trusted entity 는 AWS service 를 선택하고 Choose the service that will use this role 은 Redshift를 선택합니다. 마지막으로 Select your use case 는 Redshiftcustomizable을 선택한 후 Next: Permissions 버튼을 클릭합니다.  생성할 Role에는 두 개의 Policy를 Attach 합니다. Filter에 s3 를 입력하고 AmazonS3FullAccess 정책을 체크한 후, 다시 Filter에 Glue를 입력하고 AWSGlueConsoleFullAccess 정책을 체크한 후 Next: Tags를 클릭 후, 다음 Next: Review 버튼을 클릭합니다.  Role name에 redshift_role을 입력하고 Create role을 클릭하여 IAM Role 생성을 완료합니다. 차후 실습에 사용을 위해 Role ARN을 기록합니다. (arn:aws:iam:: 으로 시작)   Amazon Redshift 클러스터 생성 실습을 위하여 Redshift 클러스터를 생성합니다. 클러스터는 샘플 데이터가 있는 us-west-2 (Oregon) 와 동일한 리전에 생성해야 합니다. 또한 Redshift 의 접속을 위하여 보안 그룹 설정을 주의하여 생성 하시기 바랍니다.  AWS Management Console에서 Redshift 서비스에 접속 후 좌측 Clusters 탭을 선택 합니다. Launch cluster 버튼을 클릭하여 클러스터 생성을 시작합니다. Cluster identifier, Database name, Master user name, Master user password, Confirm password를 임의대로 차례로 입력한 뒤 Continue를 클릭합니다. (Database Port는 Default Port인 5439 사용합니다.)  Cluster identifier : redshift Database name : redshift Master user name : admin Password: **** (임의의 Redshift Master용 password 생성)    Node Configuration 부분은 default 옵션으로 진행합니다. Continue를 클릭합니다.  Additional Configuration에서 VPC security groups에는 default를 선택합니다. Available roles에는 이전 단계에 생성한 redshift_role을 선택합니다. Continue를 클릭합니다.  Review한 뒤 Launch cluster를 선택하여 클러스터를 생성합니다. Cluster Status가 available이 되면 다음으로 진행합니다.  Redshift 접속 이 과정에서는 Amazon Redshift 클러스터에 연결합니다. 저희 실습에서는 Redshift에서 기본으로 제공하는 쿼리 작성기인 Query editor를 이용합니다. 본 도구는 임시적으로 사용하는 도구이며, 실제로는 JDBC 연결을 통해 ** 적절한 데이터베이스 관리도구를 이용 **하게 됩니다. Redshift 관리 콘솔에서 Query editor를 실행 합니다.  다음 설정을 구성합니다.  Cluster: redshift Database: redshift Username: admin Password: (이전 단계에서 만든 password 입력) Connect 를 클릭    외부 테이블 생성 이 실습에서는 외부 테이블을 생성합니다. 일반 Redshift 테이블과는 달리 외부 테이블은 Amazon S3에 저장된 데이터를 참조합니다. 먼저 외부 스키마를 정의합니다. 외부 스키마는 외부 데이터 카탈로그에 있는 데이터베이스를 참조하고, 클러스터가 사용자 대신 Amazon S3에 액세스할 수 있도록 권한을 부여하는 IAM 역할 식별자(ARN)를 제공합니다.  ** INSERT-YOUR-REDSHIFT-ROLE **을 실습 2에서 생성한 redshift_role의 ARN값으로 대체하고 Query editor에서 이 명령을 실행합니다.  CREATE EXTERNAL SCHEMA spectrum\rFROM DATA CATALOG\rDATABASE 'spectrumdb'\rIAM_ROLE 'INSERT-YOUR-REDSHIFT-ROLE'\rCREATE EXTERNAL DATABASE IF NOT EXISTS\rQuery editor의 결과는 별도 정보가 표시되지 않고 \u0026ldquo;Statement completed successfully\u0026quot;라는 메시지를 수신하면, 다음 단계로 진행하십시오. 이제 spectrum 스키마에 저장될 외부 테이블을 생성합니다. Query editor에서 이 명령을 실행하여 외부 테이블을 생성합니다.  CREATE EXTERNAL TABLE spectrum.sales(\rsalesid INTEGER,\rlistid INTEGER,\rsellerid INTEGER,\rbuyerid INTEGER,\reventid INTEGER,\rdateid SMALLINT,\rqtysold SMALLINT,\rpricepaid DECIMAL(8,2),\rcommission DECIMAL(8,2),\rsaletime TIMESTAMP\r)\rROW FORMAT DELIMITED\rFIELDS TERMINATED BY '\\t'\rSTORED AS TEXTFILE\rLOCATION 's3://id-redshift-uswest2/tickit/spectrum/sales/'\rTABLE PROPERTIES ('numRows'='172000')\rQuery editor에는 아무런 정보가 표시되지 않습니다. 외부 테이블은 테이블의 목록에 표시되지 않기 때문입니다. 이 문이 Amazon S3에 있는 디렉터리를 가리키는 테이블 정의를 생성했습니다. 디렉터리에는 172,456개의 행이 있는 11MB 텍스트 파일 1개가 포함되어 있습니다. 다음은 파일 콘텐츠 샘플입니다.\n2 4 8117 11498 4337 1983 2 76.00 11.40 2008-06-06 05:00:16\r6 10 24858 24888 3375 2023 2 394.00 59.10 2008-07-16 11:59:24\r7 10 24858 7952 3375 2003 4 788.00 118.20 2008-06-26 00:56:06\r8 10 24858 19715 3375 2017 1 197.00 29.55 2008-07-10 02:12:36\r각 줄에는 수량, 가격 및 판매 날짜와 같은 판매 정보가 있습니다.\nAmazon S3에 저장된 데이터를 쿼리 이 실습에서는 외부 테이블에 대해 쿼리를 실행합니다. 이 쿼리는 Redshift Spectrum을 사용하여 Amazon S3에서 바로 데이터를 처리합니다. 다음 명령을 실행하여 S3에 저장된 행의 수를 쿼리합니다.  SELECT COUNT(*) FROM spectrum.sales\r출력값은 파일에 172,456 개의 레코드가 있음을 보여줍니다.\n다음 명령을 실행하여 외부 테이블에 저장된 데이터 샘플을 확인합니다.  SELECT * FROM spectrum.sales LIMIT 10\rS3에 저장된 탭으로 분리된 데이터가 일반 Redshift 테이블과 정확히 동일하게 표시되는 것을 확인할 수 있습니다. Spectrum은 S3에서 데이터를 읽지만 마치 Redshift가 직접 읽는 것처럼 표시합니다.또한, 쿼리는 합계 계산과 같은 일반 SQL 문을 포함할 수 있습니다. 다음 명령을 실행하여 일의 매출을 계산합니다.\nSELECT SUM(pricepaid)\rFROM spectrum.sales\rWHERE saletime::date = '2008-06-26'\rAmazon Redshift Spectrum은 임시 Amazon Redshift 테이블로 데이터를 로드할 필요 없이 Amazon S3에 저장된 데이터에 직접 쿼리를 실행합니다.또한, S3에 저장된 데이터와 Amazon Redshift에 저장된 데이터를 조인할 수 있습니다. 이를 보여주기 위해 event라는 일반 Redshift 테이블을 생성하고 이 테이블로 데이터를 로드합니다.\n 다음 명령을 실행하여 일반 Redshift 테이블을 생성합니다.event 테이블이 페이지 왼쪽의 테이블 목록에 표시됩니다.  CREATE TABLE event(\reventid INTEGER NOT NULL DISTKEY,\rvenueid SMALLINT NOT NULL,\rcatid SMALLINT NOT NULL,\rdateid SMALLINT NOT NULL SORTKEY,\reventname VARCHAR(200),\rstarttime TIMESTAMP\r)\rINSERT-YOUR-REDSHIFT-ROLE을 이전 단계에서 생성한 redshift_role의 ARN 값을 대체하고 Query editor에서 이 명령을 실행하여 데이터를 events 테이블로 로드합니다. 약 30초 가량의 로딩 시간이 소요됩니다.  COPY event\rFROM 's3://id-redshift-uswest2/tickit/allevents_pipe.txt'\rIAM_ROLE 'INSERT-YOUR-REDSHIFT-ROLE'\rDELIMITER '|'\rTIMEFORMAT 'YYYY-MM-DD HH:MI:SS'\rREGION 'us-west-2'\r다음 명령을 실행하여 event 데이터의 샘플을 확인합니다.  SELECT * FROM event LIMIT 10\r이제 이 새로운 event 테이블의 데이터 (Redshift 저장 데이터)와 외부 sales 테이블의 데이터 (S3 저장데이터)를 조인하는 쿼리를 실행할 수 있습니다.\n다음의 명령을 통해 로컬 event 테이블과 외부 sales 테이블을 조인하여 상위 10개 이벤트의 총 매출을 확인합니다.  SELECT TOP 10\rspectrum.sales.eventid,\rSUM(spectrum.sales.pricepaid)\rFROM spectrum.sales, event\rWHERE spectrum.sales.eventid = event.eventid\rAND spectrum.sales.pricepaid \u0026gt; 30\rGROUP BY spectrum.sales.eventid\rORDER BY 2 DESC\r이 쿼리는 가격이 30 USD 이상의 이벤트 별 (Redshift 저장 데이터) 로 그룹화된 총 매출 (S3 저장 데이터) 을 나열합니다.\n다음의 명령을 실행하여 위의 쿼리에 대한 쿼리 플랜을 확인합니다.이 쿼리 플랜은 Redshift가 해당 쿼리를 어떻게 실행할 지 보여줍니다. Amazon S3에 있는 데이터에 대해 S3 Seq Scan, S3 HashAggregate 및 S3 Query Scan 단계가 실행됩니다.  EXPLAIN\rSELECT TOP 10\rspectrum.sales.eventid,\rSUM(spectrum.sales.pricepaid)\rFROM spectrum.sales, event\rWHERE spectrum.sales.eventid = event.eventid\rAND spectrum.sales.pricepaid \u0026gt; 30\rGROUP BY spectrum.sales.eventid\rORDER BY 2 DESC\r파티션된 데이터 사용 외부 테이블은 디렉터리로 사전에 파티션 될 수 있으며, 각 디렉터리는 데이터 하위집합을 포함합니다.데이터를 파티션할 때 파티션 키를 필터링하여 Redshift Spectrum이 스캔하는 데이터의 양을 제한할 수 있습니다.시간에 따라 데이터를 파티션하는 것이 일반적입니다. 예를 들어, 년, 월, 일 및 시간에 따라 파티션할 수 있습니다. 데이터가 여러 소스에서 수신되는 경우, 데이터 소스 식별자와 날짜로 파티션할 수 있습니다.다음은 분할된 데이터를 보여주는 디렉터리 목록으로, 디렉터리에 월별로 파티션된 S3 파일 집합을 표시합니다. (참고: AWS Cli가 설치된 로컬 머신에서 확인 가능 합니다.)\n$ aws s3 ls s3://id-redshift-uswest2/tickit/spectrum/sales_partition/\rPRE saledate=2008-01/\rPRE saledate=2008-02/\rPRE saledate=2008-03/\rPRE saledate=2008-04/\rPRE saledate=2008-05/\rPRE saledate=2008-06/\rPRE saledate=2008-07/\rPRE saledate=2008-08/\rPRE saledate=2008-09/\rPRE saledate=2008-10/\rPRE saledate=2008-11/\r이제 이 데이터를 사용하는 외부 테이블을 정의 합니다.다음 명령을 실행하여 파티션된 데이터에 따라 새로운 sales_partitioned 테이블을 정의합니다.\nCREATE EXTERNAL TABLE spectrum.sales_partitioned(\rsalesid INTEGER,\rlistid INTEGER,\rsellerid INTEGER,\rbuyerid INTEGER,\reventid INTEGER,\rdateid SMALLINT,\rqtysold SMALLINT,\rpricepaid DECIMAL(8,2),\rcommission DECIMAL(8,2),\rsaletime TIMESTAMP\r)\rPARTITIONED BY (saledate DATE)\rROW FORMAT DELIMITED\rFIELDS TERMINATED BY '|'\rSTORED AS TEXTFILE\rLOCATION 's3://id-redshift-uswest2/tickit/spectrum/sales_partition/'\rTABLE PROPERTIES ('numRows'='172000')\r(이 쿼리를 실행하면 화면에 응답이 표시되지 않지만, 테이블 정의가 생성됩니다.)salesdate 필드에 따라 테이블이 파티션됨을 Redshift Spectrum에 알려주는 문이 추가되었습니다.그런 다음 Redshift Spectrum은 기존 파티션에 대한 정보를 받아야 어떤 디렉터리를 사용할 지 알 수 있습니다.다음 명령을 실행하여 파티션을 추가합니다.\nALTER TABLE spectrum.sales_partitioned ADD PARTITION (saledate='2008-01-01') LOCATION 's3://id-redshift-uswest2/tickit/spectrum/sales_partition/saledate=2008-01/'\rPARTITION (saledate='2008-02-01') LOCATION 's3://id-redshift-uswest2/tickit/spectrum/sales_partition/saledate=2008-02/'\rPARTITION (saledate='2008-03-01') LOCATION 's3://id-redshift-uswest2/tickit/spectrum/sales_partition/saledate=2008-03/'\rPARTITION (saledate='2008-04-01') LOCATION 's3://id-redshift-uswest2/tickit/spectrum/sales_partition/saledate=2008-04/'\rPARTITION (saledate='2008-05-01') LOCATION 's3://id-redshift-uswest2/tickit/spectrum/sales_partition/saledate=2008-05/'\rPARTITION (saledate='2008-06-01') LOCATION 's3://id-redshift-uswest2/tickit/spectrum/sales_partition/saledate=2008-06/'\rPARTITION (saledate='2008-07-01') LOCATION 's3://id-redshift-uswest2/tickit/spectrum/sales_partition/saledate=2008-07/'\rPARTITION (saledate='2008-08-01') LOCATION 's3://id-redshift-uswest2/tickit/spectrum/sales_partition/saledate=2008-08/'\rPARTITION (saledate='2008-09-01') LOCATION 's3://id-redshift-uswest2/tickit/spectrum/sales_partition/saledate=2008-09/'\rPARTITION (saledate='2008-10-01') LOCATION 's3://id-redshift-uswest2/tickit/spectrum/sales_partition/saledate=2008-10/'\rPARTITION (saledate='2008-11-01') LOCATION 's3://id-redshift-uswest2/tickit/spectrum/sales_partition/saledate=2008-11/'\rPARTITION (saledate='2008-12-01') LOCATION 's3://id-redshift-uswest2/tickit/spectrum/sales_partition/saledate=2008-12/'\r이제 특정 salesdate를 사용하는 모든 쿼리에서 해당 날짜와 관련된 디렉터리만 스캔합니다.비교를 위해 2개의 서로 다른 데이터 소스에 쿼리를 실행합니다. 원래 sales 테이블에 다음 명령을 실행하고 실행에 걸리는 시간을 기록합니다.   SELECT TOP 10\rspectrum.sales.eventid,\rSUM(pricepaid)\rFROM spectrum.sales, event\rWHERE spectrum.sales.eventid = event.eventid\rAND pricepaid \u0026gt; 30\rAND date_trunc('month', saletime) = '2008-12-01'\rGROUP BY spectrum.sales.eventid\rORDER BY 2 DESC\r파티션된 데이터에 다음의 명령을 실행하고 실행에 걸리는 시간을 기록합니다.   SELECT TOP 10\rspectrum.sales_partitioned.eventid,\rSUM(pricepaid)\rFROM spectrum.sales_partitioned, event\rWHERE spectrum.sales_partitioned.eventid = event.eventid\rAND pricepaid \u0026gt; 30\rAND saledate = '2008-12-01'\rGROUP BY spectrum.sales_partitioned.eventid\rORDER BY 2 DESC\r두번째 쿼리가 더 빠르게 실행되는 것을 확인합니다. 이는 Amazon S3에서 읽는 데이터가 더 적기 때문입니다. 데이터 볼륨이 클수록 실행 속도의 차이가 더 분명해 집니다. (다만 본 예제와 같이 데이터량이 작은 경우 그 차이는 미비합니다.) 또한, Amazon S3에서 읽는 데이터량에 따라 Redshift Spectrum에 대한 요금이 부과되므로, 쿼리 실행 비용도 줄어듭니다.파티션에 대한 정보는 SVV_EXTERNAL_PARTITIONS 시스템 뷰에서 확인할 수 있습니다. 다음의 명령을 실행하여 sales_partitioned 테이블에 대한 파티션을 확인합니다.  SELECT *\rFROM SVV_EXTERNAL_PARTITIONS\rWHERE tablename = 'sales_partitioned'\r실습 완료 이번 실습을 완료하였습니다. 비용 발생을 최소화 하기 위하여 실습환경을 정리하십시오.\n "
},
{
	"uri": "/ServicesAnalytics/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/ServicesAnalytics/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/ServicesAnalytics/credits/",
	"title": "크레딧",
	"tags": [],
	"description": "",
	"content": "컨텐츠 최초제작 기여자  AWS CS Korea SA  anhyobin@    컨텐츠 업데이트 기여자   AWS CS Korea SA\n hyouk@ kseongmo@ sehyul@    AWS WWPS Korea SA\n inhwan@ seokjae@ donghyun@ jungwons@    컨텐츠 웹 변환 기여자  AWS WWPS Korea SA  inhwan@ seokjae@ donghyun@ jungwons@    소프트웨어 기여자 오픈 소스 소프트웨어를 더 좋게 만들어 주시는 기여자 분들께 감사드립니다!\n@vjeantet의 docdock, hugo-theme-learn의 분기에 대한 업적에 특별히 감사드립니다. 이 테마의 v2.0.0은 그의 작업으로 부터 영감 받았습니다.\n패키지와 라이브러리  mermaid - generation of diagram and flowchart from text in a similar manner as markdown font awesome - the iconic font and CSS framework jQuery - The Write Less, Do More, JavaScript Library lunr - Lunr enables you to provide a great search experience without the need for external, server-side, search services\u0026hellip; horsey - Progressive and customizable autocomplete component clipboard.js - copy text to clipboard highlight.js - Javascript syntax highlighter modernizr - A JavaScript toolkit that allows web developers to use new CSS3 and HTML5 features while maintaining a fine level of control over browsers that don\u0026rsquo;t support  도구  Netlify - Continuous deployement and hosting of this documentation Hugo  "
}]