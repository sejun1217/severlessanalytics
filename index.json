[
{
	"uri": "/serverless/intro/",
	"title": "소개",
	"tags": [],
	"description": "",
	"content": "이 워크샵을 통해 AWS 기반의 데이터 분석을 실습합니다.준비사항  AWS 계정: EC2, S3, IAM, Glue, Athena, QuickSight, Aurora, Lambda, API Gateway 자원을 생성할 수 있는 권한이 필요합니다. AWS 리전: 이번 실습은 버지니아 (us-east-1) 리전에서 실행합니다. 브라우저: 최신 버전의 크롬, 파이어폭스를 사용하세요.  Contact Us AWS 서비스에 관한 질문은 AWS Support나 담당 AM을 통해서 문의해 주시고 본 워크샵의 발표자료에 관한 질문 사항은 아래의 email 링크를 통해 문의해 주시면 감사하겠습니다. 김세준 Solutions Architect: sejun@amazon.com 고영경 Solutions Architect: youngkko@amazon.com   "
},
{
	"uri": "/serverless/",
	"title": "홈",
	"tags": [],
	"description": "",
	"content": "AWS Serverless Analytics 서비스 워크샵   "
},
{
	"uri": "/serverless/lab0/",
	"title": "실습0. 사전 준비",
	"tags": [],
	"description": "",
	"content": "실습 소개 본격적인 Lab 시작에 앞서 구성에 필요한 IAM User, EC2, S3를 생성 및 구성합니다.\n\r IAM User 생성 Lab 전체에서 사용할 IAM User를 생성합니다.  AWS Management Console에 로그인 한 뒤 IAM 서비스에 접속합니다.\n  왼쪽 메뉴에서 Users를 선택합니다.   Add user 버튼을 클릭하여 사용자 추가 페이지로 들어갑니다.\n  User name에 \u0026lt;원하는 사용자 이름\u0026gt; 을 입력하고, Access type에 Programmatic access와 AWS Management Console access 둘 모두를 선택합니다. Console password에 \u0026lt;원하는 패스워드\u0026gt;를 입력하고, 마지막 Require password reset의 체크는 해제합니다.   Next: Permissions 버튼을 클릭하고 Attach existing policies directly를 선택한 뒤 AdministratorAccess 권한을 추가해줍니다.   Next: Review 버튼을 클릭하고 정보를 확인한 뒤 Create user 버튼을 클릭하여 사용자 생성을 완료합니다.   Download.csv 버튼을 클릭하여 생성한 사용자의 정보를 다운 받습니다. EC2 설정에 꼭 필요한 파일이므로 기억하기 쉬운 위치에 저장합니다.   AWS Management Console에 로그인 한 뒤 IAM 서비스에 접속합니다.\n  왼쪽 메뉴에서 Roles를 선택합니다. 상단의 Create role 버튼 클릭하여 role 생성 page로 들어갑니다.   AWS service 선택, EC2 service 를 선택 합니다. Next: Permissions를 선택합니다.   Filter에 \u0026ldquo;SSM\u0026rdquo; 을 입력하고, AmazonEC2RoleforSSM를 선택합니다. Next:Tags 버튼을 클릭해서 tags 입력 부분으로 들어가서, Next:Review 버튼을 클릭합니다.   Role name에는 EC2RoleforSSM을 입력하고, Create role 버튼을 클릭합니다.   EC2 생성 Lab에서 데이터를 실시간으로 발생시킬 EC2 인스턴스를 생성합니다. AWS Management Console에서 EC2 서비스에 접속합니다. 우측 상단에서 Region은 **미국 동부(버지니아 북부)**를 선택합니다. Launch Instance를 선택하여 새로운 인스턴스 생성을 시작합니다.  Step 1: Choose an Amazon Machine Image (AMI) 화면에서 Amazon Linux AMI 2018.03.0 (HVM), SSD Volume Type - ami-0761dd91277e34178 를 선택합니다.  Step 2 : Choose an Instance Type 화면에서 인스턴스 타입은 t2.micro를 선택합니다. Next: Configure Instance Details 을 클릭합니다.  Step 3: Configure Instance Details 화면에서 IAM role에서 미리 생성한 EC2RoleforSSM을 선택 합니다.  Step 3: Configure Instance Details 화면에서 Advanced Details을 클릭하고 아래 userdata를 복사하여 붙여 넣습니다.  #include\rhttps://s3.amazonaws.com/immersionday-bigdata-v20180731/userdata.sh\r8. Step 4: Add Storage 화면에서 기본값을 그대로 두고 Next: Add Tags를 클릭합니다. 9. Step 5: Add Tags 화면에서 Add Tag 버튼을 한 번 클릭한 뒤, Key/Value : Name/AnalyticsStream 를 입력하고 Next: Configure Security Group 을 클릭합니다. 10. Step 6: Configure Security Group 화면에서 Security Group에 필요한 정보를 입력한 후 Review and Launch를 클릭합니다. * Security Group Name : bastion * Description : SG for bastion * Type : SSH * Protocol : TCP * Port Range : 22 * Source : 0.0.0.0/0 11. Step 7: Review Instance Launch 화면에서 Launch를 클릭합니다. 12. EC2 Instance에 접속하기 위한 Key pair를 생성합니다. Create a new key pair를 선택하고 Key pair name은 analytics-hol 을 입력한 후 Download Key Pair를 클릭합니다. 13. Key Pair를 PC의 임의 위치에 저장한 후 Launch Instances를 클릭합니다. (인스턴스 기동에 몇 분이 소요될 수 있습니다.) 14. (MacOS 사용자) 다운로드 받은 Key Pair 파일의 File Permission을 400으로 변경합니다.\n$ chmod 400 ./analytics-hol.pem\r$ ls -lat analytics-hol.pem\r-r-------- 1 ****** ****** 1692 Jun 25 11:49 analytics-hol.pem\rEC2 설정 - System manager (seesion manager를 통해 ec2 ssh 접속) 생성한 EC2 인스턴스가 다른 AWS 리소스에 접근 및 제어할 수 있도록 다음과 같이 구성합니다. AWS Management Console에서 Systems Manager 서비스에 접속합니다. 왼쪽 하단의 Session Manager 를 선택 합니다.  Session manager console에서 Start session 버튼을 클릭합니다.  앞에서 생성한 AnalyticsStream instance를 선택하고, 하단의 Start session 버튼을 클릭합니다.  instance session에 들어가서 /home/ec2-user/ 에 있는 3개의 files을 /home/ssm-user/ 로 copy 합니다.  sudo cp /home/ec2-user/redshift.py /home/ssm-user/\rsudo cp /home/ec2-user/firehose.py /home/ssm-user/\rsudo cp /home/ec2-user/banking_loss.csv /home/ssm-user/\rUser Data를 통해 3가지 파일(banking_loss.csv, firehose.py, redshift.py)이 잘 다운로드 받아졌는지 확인합니다.  ls /home/ssm-user/\rAWS의 다른 리소스 접근을 위해 AWS Configure를 진행합니다. 이때 앞서 생성한 IAM User 데이터를 활용합니다. 이전에 다운로드 받은 .csv 파일을 열어 Access key ID와 Secret access key를 확인하고 입력합니다. region은 us-west-2 로 입력 합니다.  aws configure\rAWS Access Key ID [None]: \u0026lt;Access key ID\u0026gt;\rAWS Secret Access Key [None]: \u0026lt;Secret access key\u0026gt;\rDefault region name [None]: us-west-2\rDefault output format [None]:\r설정이 완료 되었다면, aws configure list 로 다음과 같이 입력하신 정보가 마스킹 되어 보이게 됩니다.  aws configure list\rS3 Bucket 생성 발생한 실시간 데이터를 저장할 S3 Bucket을 생성합니다. AWS Management Console에서 S3 서비스에 접속합니다. Create bucket 버튼을 클릭하고 다음과 같은 정보를 입력한 뒤 Create 버튼을 클릭하여 bucket을 생성합니다.             Bucket name analytics-workshop-[개인식별자] (예 : analytics-workshop-foobar)   Region 미국 서부 (오레곤)   그 외 default     "
},
{
	"uri": "/serverless/lab1/",
	"title": "실습1. Glue, Athena, QuickSight로 수집한 S3의 데이터 분석",
	"tags": [],
	"description": "",
	"content": "실습 소개 이번 실습에서는 실시간 또는 batch 방식으로 수집되어 S3에 저장한 데이터를 Glue, Athena, QuickSight 를 이용해 분석해봅니다.\n\r 실습 아키텍처  AWS Glue AWS Glue는 완전관리형 ETL(Extract, Transform, Load) 엔진입니다. 이번 실습에서는 실시간 또는 batch 방식으로 수집되어 S3에 저장한 데이터를 Glue, Athena, QuickSight 를 이용해 분석해봅니다. Hive 호환 External Table을 만들고 Athena, QuickSight로 데이터를 분석하고 시각화 합니다. 이 실습에서는 간단히 다음 커맨드로 batch 방식 수집을 대신하겠습니다.. AWS Glue에서 ETL Job을 수행하기 위해서는 특정 IAM 역할이 필요합니다. IAM 서비스에 접속합니다. https://console.aws.amazon.com/iam 좌측 메뉴에서 정책을 클릭하고 정책 생성\r\r버튼을 클릭합니다. 정책 생성 화면에서 JSON 탭을 클릭하고 아래 Policy를 복사하여 붙여 넣고 정책 검토\r\r를 클릭합니다.  {\r\u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;,\r\u0026quot;Statement\u0026quot;: [\r{\r\u0026quot;Action\u0026quot;: [\r\u0026quot;s3:*\u0026quot;,\r\u0026quot;ec2:*\u0026quot;,\r\u0026quot;iam:*\u0026quot;,\r\u0026quot;glue:*\u0026quot;,\r\u0026quot;logs:*\u0026quot;\r],\r\u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;,\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;\r}\r]\r}\r정책 검토 화면에서 이름은 GluePolicy를 입력하고 정책 생성\r\r버튼을 클릭합니다. 좌측 메뉴에서 역할을 클릭하고 역할 만들기\r\r버튼을 클릭합니다. 이 역할을 사용할 서비스 선택 에서 Glue를 클릭하고 다음: 권한\r\r버튼을 클릭합니다. 검색 입력창에 방금 만든 GluePolicy를 입력하고 GluePolicy를 선택하고 다음: 태그\r\r버튼을 클릭합니다. 태그 추가 화면에서 다음: 검토\r\r버튼을 클릭합니다 검토 화면에서 역할 이름은 GlueRole을 입력하고 역할 만들기\r\r버튼을 클릭합니다. 좌측 역할 을 클릭하고 GlueRole이 정상적으로 생성되었는지 확인합니다. AWS Glue (https://console.aws.amazon.com/glue) 로 이동합니다. 이전 실습에서 생성한 S3내 source 폴더 아래 파일 들을 크롤링하여 메타스토어 테이블을 만듭니다.데이터 카탈로그 \u0026gt; 데이터베이스 를 클릭하고 데이터베이스 추가\r\r를 클릭합니다.  데이터베이스 이름 은 workshop 를 입력하고 생성\r\r버튼을 클릭합니다. 데이터 카탈로그 \u0026gt; 데이터베이스 \u0026gt; 테이블 을 클릭하고 테이블 추가\r\r\u0026gt; 크롤러를 사용하여 테이블 추가 를 클릭합니다.  크롤러 이름에 TestCrawler 입력 후 다음\r\r을 클릭합니다. Crawler source type에 Data stores 를 선택 후, 다음\r\r을 클릭합니다. 크롤링 할 S3 버킷 및 폴더를 지정합니다. 포함 경로 에 s3://analytics-workshop-[개인식별자]/source 을 입력하거나 탐색기 버튼을 클릭하여 analytics-workshop-[개인식별자] 버킷 아래 source 폴더를 선택\r\r한 후 다음\r\r을 클릭합니다. 다른 데이터 스토어 추가는 아니요를 선택하고 다음\r\r을 클릭합니다. IAM 역할 선택 화면에서 기존 IAM 역할 선택을 선택하고 앞서 생성한 GlueRole을 선택한 후 다음\r\r을 클릭합니다. 크롤러는 온디맨드 방식으로도 실행할 수 있고, 배치 방식으로도 실행할 수 있습니다. 실습에서는 온디맨드 방식으로 실행합니다. 빈도는 온디맨드 실행을 선택하고 다음\r\r을 클릭합니다. 데이터베이스는 앞서 생성한 workshop를 선택한 후 다음\r\r을 클릭합니다. 크롤러 설정을 모두 확인한 후 마침\r\r을 클릭합니다. 데이터 카탈로그 \u0026gt; 크롤러 에서 생성한 TestCrawler를 선택한 후 크롤러 실행\r\r을 클릭합니다.  크롤러는 S3에 저장된 파일을 분석하고 테이블을 생성합니다. 크롤링이 끝난 후 테이블 1개 (테이블명 : source)가 생성되었음을 확인합니다.  데이터베이스 \u0026gt; 테이블 에서 방금 생성된 source 테이블을 클릭하여 테이블 구조를 확인합니다. Kinesis Firehose가 s3://analytics-workshop-[개인식별자]/source 에 저장한 JSON 파일 포맷 스트림 데이터를 Parquet 파일 포맷으로 변경하고, 일부 컬럼의 데이터 타입을 변경한 후 s3://analytics-workshop-[개인식별자]/parquet 폴더에 파일을 저장하는 Glue ETL 작업을 생성하겠습니다.  웹 브라우저에 접속하여 source.tgz 파일을 다운로드 받습니다 s3://analytics-workshop-[개인식별자]/source 에 source.taz 파일을 업로드합니다   우선 ETL후 결과 파일이 저장될 폴더를 생성합니다. S3 콘솔로 로그인한 후 analytics-workshop-[개인식별자] 버킷를 선택하고 parquet 폴더를 생성합니다.  Glue 콘솔(https://console.aws.amazon.com/glue) 에서 ETL \u0026gt; 작업을 선택한 후 작업 추가\r\r를 클릭합니다. 작업 속성 구성 화면에서 이름은 TestJob을 입력하고, IAM 역할은 앞서 생성한 GlueRole을 선택합니다. 고급 속성 에서 작업 북마크를 활성화하여 Glue가 마지막으로 처리한 데이터를 기억하게 합니다. 다음\r\r을 클릭합니다. 데이터 원본 선택 화면에서 source를 선택하고 다음\r\r을 클릭합니다.  변환 유형 선택 화면에서 스키마 변경을 선택하고 다음\r\r을 클릭합니다. 데이터 대상 선택 화면에서 데이터 대상에서 테이블 생성을 선택하고 데이터 스토어는 Amazon S3, 형식은 Parquet, 대상 경로는 s3://analytics-workshop-[개인식별자]/parquet 을 입력합니다. 다음\r\r을 클릭합니다.  occurencestartdate, discoverydate 컬럼의 데이터 형식을 date로 변경한 후 작업 저장 및 스크립트 편집\r\r을 클릭합니다.  스크립트 내용을 검토한 후 상단의 작업 실행\r\r버튼을 클릭하여 ETL을 시작합니다. 작업 완료까지 수 분이 소요될 수 있습니다. Glue ETL 작업이 끝나면 s3://analytics-workshop-[개인식별자]/parquet 폴더에 parquet 타입 파일이 생성됩니다. Glue ETL이 변환한 Parquet 파일을 크롤링하여 테이블을 생성합니다. AWS Glue \u0026gt; 크롤러 \u0026gt; TestCrawler를 선택하고 작업 \u0026gt; 크롤러 편집 를 선택합니다.  크롤러 정보 추가 화면에서 다음\r\r을 클릭합니다. Specify crawler source type 화면에서 다음\r\r을 클릭합니다. 데이터 스토어 추가 화면에서 다음\r\r을 를 클릭합니다. 다른 데이터 스토어 추가 화면에서 예를 선택한 후 다음\r\r을 클릭합니다. 데이터 스토어 추가 화면에서 포함 경로에 s3://analytics-workshop-[개인식별자]/parquet 을 입력한 후 다음\r\r을 클릭합니다.  다른 데이터 스토어 추가 화면에서 다음\r\r을 클릭합니다. IAM 역할 선택 화면에서 다음\r\r을 클릭합니다. 이 크롤러의 일정 생성 화면에서 다음\r\r을 클릭합니다. 크롤러의 출력 구성 화면에서 다음\r\r을 클릭합니다. 리뷰 화면에서 마침\r\r을 클릭합니다. TestCrawler를 선택하고 크롤러 실행\r\r을 클릭합니다.  크롤러 실행 후 parquet 테이블이 생성됩니다.   Amazon Athena  Athena를 이용하여 테이블 데이터를 조회할 수 있습니다. Glue 크롤러가 만든 parquet 테이블을 클릭하고 작업 \u0026gt; 데이터 보기를 클릭합니다.  Athena 콘솔이 열리면, 시작하기\r\r를 클릭합니다. set up a query result location in Amazon S3 를 클릭합니다.  쿼리 결과 위치에 s3://analytics-workshop-[개인식별자]/query-result/ 를 입력하고 저장\r\r을 클릭합니다.  다시 Glue 크롤러가 만든 parquet 테이블을 클릭하고 작업 \u0026gt; 데이터 보기를 클릭합니다. Athena 콘솔이 열리면 아래 쿼리를 수행합니다.  SELECT * FROM \u0026quot;workshop\u0026quot;.\u0026quot;parquet\u0026quot; limit 10;\r7. ANSI 표준 SQL문을 통해 S3내 데이터를 조회할 수 있습니다.\nSELECT region, status, count(*) as \u0026quot;COUNT\u0026quot;\rFROM \u0026quot;workshop\u0026quot;.\u0026quot;parquet\u0026quot;\rGROUP BY region, status\rORDER BY region;\rAmazon QuickSight  이번에는 Amazon QuickSight를 통해 parquet 테이블 데이터를 시각화 해 보도록 하겠습니다. QuickSight 콘솔로 이동합니다. https://quicksight.aws.amazon.com QuickSight에 가입하기 위해 Sign up for QuickSight\r\r버튼을 클릭합니다.  Standard 에디션을 선택한 후 계속\r\r버튼을 클릭합니다. QuickSight 리전은 **US-West (Oregon)**을 선택하고 QuickSight 계정 이름은 임의로 지정(중복될 경우 계정이 생성되지 않습니다) 하고 알림 이메일 주소는 개인 Email 주소를 입력합니다. QuckSight가 S3에 접근해야 하므로, Choose S3 buckets를 클릭하여 analytics-workshop-[개인식별자] 을 선택한 후 완료\r\r를 클릭합니다.  계정이 생성된 후 Amazon QuickSight로 이동\r\r버튼을 클릭합니다. 좌측 상단 새 분석\r\r을 클릭합니다.  \r새 데이터 세트\r\r버튼을 클릭합니다.  Athena를 클릭하고 팝업 창의 데이터 원본 이름에 analytics-quicksight를 입력(임의의 값 입력 가능)하고 데이터 원본 생성\r\r버튼을 클릭합니다.  테이블 선택에서 데이터베이스는 workshop, 테이블은 parquet를 선택하고 선택\r\r버튼을 클릭합니다.  \rVisualize\r\r버튼을 클릭한 후 parquet 테이블 데이터가 QuickSight SPICE 엔진에 로딩 되었는지 확인합니다.  발생년도 별 Business 수를 시각화 해 보겠습니다. 좌측 필드 목록에서 occurrencestartdate, business 필드를 차례대로 클릭합니다. 시각적 객체 유형은 세로 막대 차트를 선택합니다.  그래프 하단 occurrencestartdate 를 클릭하고 집계: 일을 년으로 변경합니다.  연도별로 데이터가 집계 되었습니다.  방금 만든 대시보드를 다른 사용자에게 공유해 보겠습니다. 우측 상단 사용자 아이콘을 클릭하고 QuickSight 관리를 클릭합니다. \r사용자 초대\r\r버튼을 클릭한 후 임의의 사용자 계정명(BI_user01)을 입력한 후 우측    \r버튼을 클릭합니다. 이메일은 다른 사용자의 이메일 주소를 입력하고 역할은 작성자, IAM 사용자는 아니요를 선택한 후 초대\r\r버튼을 클릭합니다.  사용자는 다음과 같은 초대 이메일을 받고 초대를 수락하려면 클릭하십시오\r\r를 클릭하면 계정 생성 메뉴에서 비밀번호를 변경할 수 있습니다.  QuickSight 화면으로 돌아가서 우측 상단의 공유 \u0026gt; 분석 공유를 클릭합니다.  BI_user01을 선택한 후 공유\r\r버튼을 클릭합니다.  사용자는 다음과 같은 이메일을 수신합니다. Click to View\r\r를 클릭하여 분석 결과를 확인할 수 있습니다.    "
},
{
	"uri": "/serverless/lab2/",
	"title": "실습2. API Gateway, Lambda로 Aurora 데이터 가져오기",
	"tags": [],
	"description": "",
	"content": "실습 소개 이번 실습 에서는 API Gateway, Lambda를 이용해서 Aurora에 저장된 데이터를 가져오는 실습을 진행할 예정입니다.\n\r 실습 아키텍처  CloudFormation으로 Aurora Serverless 구성 CloudFormation은 사용하면 텍스트 파일 또는 프로그래밍 언어로 전체 인프라와 애플리케이션 리소스를 모델링할 수 있습니다. 이를 통하여 VPC 및 Aurora Serverless몇 번의 클릭 만으로 구성 할 수 있습니다.\n AWS Management Console에 로그인 합니다. (Region : us-east-1) AWS Cloudformation for Aurora Serverless 접속합니다. 기본 설정을 확인 합니다. VPC 및 Subnet CIDR이 기존에 생성되어 있는 것과 겹치지 않으면, 변경 하지 않고, 화면 제일 하단에 있는 체크 박스 두개를 모두 선택한 후 스택 생성\r\r버튼을 클릭합니다.  \r스택 생성\r\r버튼을 누르면 아래와 같은 화면으로 리다이렉트 됩니다. 모든 Stack이 CREATE_COMPLETE 될때 까지 기다립니다.(약 10분 ~ 20분 소요)    Query Editor를 이용하여 간단한 쿼리 수행 CloudFormation으로 Aurora Serverless를 프로비져닝 하였습니다. Query Editor를 이용하여 Sample Query를 수행합니다. 그리고 집계 테이블을 가져오기 위한 Glue ETL 작업을 수행하기 전에 Aurora에 테이블을 생성 합니다.\n RDS Console에 접속합니다. 좌측 메뉴에서 Query Editor를 선택합니다.  생성된 Aurora Cluster my-aurora-cluster 를 선택하고, 데이터베이스 사용자 이름은 새 데이터베이스 자격 증명 추가를 선택 합니다.  옵션을 아래와 같이 입력 합니다.\r데이터베이스에 연결\r\r버튼을 누릅니다.  데이터베이스 사용자 이름 입력: MyAdmin 데이터베이스 암호 입력: Init12345 데이터베이스 이름 입력: mydb    연결 중\u0026hellip; 에서 편집기로 리다이렉트 됩니다. 생성 된 Sample Query를 수행 하기 위해,\r실행\r\r버튼을 누릅니다. Aurora Serverless 이기 때문에 약간 delay가 발생할 수 있습니다.  select * from information_schema.tables;\r7. 테스트 하기 위한 간단한 테이블을 생성합니다. 아래 쿼리를 수행 후 실행\r\r버튼을 누릅니다. 실행 후 Status가 성공 인지 확인 합니다.\nCREATE TABLE user_table(name VARCHAR(255), role VARCHAR(255));\rINSERT INTO user_table(name, role) VALUES(’abc', ‘admin’);\rINSERT INTO user_table(name, role) VALUES(’def', ‘operator’);\r AWS Lambda 생성 Aurora에 저장된 데이터를 가지고 올 수 있는 AWS Lambda 함수를 생성합니다.\n Lambda를 생성하기 전에 Aurora Serverless를 접속하기 위한 접속 정보를 확인 합니다.  cluster arn 확인 하는 방법  Aurora 데이터베이스 Console에 접속합니다. 생성된 my-aurora-cluster 클릭 합니다.  구성 탭을 선택하고 cluster arn 정보를 확인 합니다.(e.g arn:aws:rds:us-east-1:773728249266:cluster:my-aurora-cluster)    secret arn 확인 하는 방법  Secrets Manager Console에 접속합니다. Aurora 접속시 생성된 보안 암호 이름 을 선택합니다  보안 암호 ARN을 확인 합니다 (e.g arn:aws:secretsmanager:us-east-1:773728249266:secret:rds-db-credentials/cluster-NE5IADIOTWINCMB4JMZMV3KVDQ/MyAdmin-S7nkrf)      Lambda Console에 접속합니다. \r함수 생성\r\r버튼을 누릅니다.  새로 작성 상태에서, 함수 이름은 MyAurora, 런타임은 Python 3.7 을 선택하고, 함수 생성\r\r버튼을 누릅니다.  함수 코드를 아래 코드로 변경 합니다. cluster_arn 과 secret_arn은 확인한 정보로 변경 합니다. 변경 후 저장\r\r버튼을 누릅니다.  import boto3 import json # Update your cluster and secret ARNs cluster_arn = \u0026#39;\u0026lt;arn:aws:rds:us-east-1:~\u0026gt;\u0026#39; secret_arn = \u0026#39;\u0026lt;arn:aws:secretsmanager:~\u0026gt;\u0026#39; def lambda_handler(event, context): operation = event[\u0026#39;operation\u0026#39;] if \u0026#39;get\u0026#39; == operation: queryResult = get_rds_data_api() elif \u0026#39;para\u0026#39; == operation: queryResult = get_para_rds_data_api(event[\u0026#39;payload\u0026#39;][\u0026#39;role\u0026#39;]) return queryResult def get_rds_data_api(): rds_data = boto3.client(\u0026#39;rds-data\u0026#39;) sql = \u0026#34;\u0026#34;\u0026#34; SELECT * FROM user_table \u0026#34;\u0026#34;\u0026#34; response = rds_data.execute_statement( resourceArn = cluster_arn, secretArn = secret_arn, database = \u0026#39;mydb\u0026#39;, sql = sql) return response def get_para_rds_data_api(payload): rds_data = boto3.client(\u0026#39;rds-data\u0026#39;) sql = \u0026#34;\u0026#34;\u0026#34; SELECT * FROM user_table where role = :para1 \u0026#34;\u0026#34;\u0026#34; message_value = payload para1 = [{\u0026#39;name\u0026#39;:\u0026#39;para1\u0026#39;, \u0026#39;value\u0026#39;:{\u0026#39;stringValue\u0026#39;: f\u0026#39;{message_value}\u0026#39;}}] response = rds_data.execute_statement( resourceArn = cluster_arn, secretArn = secret_arn, database = \u0026#39;mydb\u0026#39;, sql = sql, parameters = para1) return response 5. 수행시간이 긴 Query 를 위해 Lambda 수행 제한시간을 1분으로 늘려 줍니다. 하단의 기본 설정 에 편집을 선택 합니다 6. 제한 시간을 1 분으로 변경하고, 저장\r\r버튼을 누릅니다. 7. Lambda가 Aurora를 접속 할 수 있도록, 실행 권한을 부여합니다. 8. 우측 상단에 권한 탭을 선택 후, 역할 이름 아래 있는 링크를 클릭 합니다. 9. IAM 콘솔로 리다이렉트 됩니다. 정책 연결\r\r버튼을 누릅니다. 10. AmazonRDSDataFullAccess 정책을 검색한 후 정책 연결\r\r버튼을 누릅니다.  Amazon API Gateway - REST API 생성 REST API Request가 들어오면 이전에 생성한 Lambda 함수를 실행하는 API를 생성합니다\n API Gateway Console에 접속합니다. REST API 생성을 위해 구축\r\r버튼을 누릅니다.  새 API 선택 합니다. API 이름은 MyApi 로 입력합니다.\rAPI 생성\r\r버튼을 누릅니다.  \r작업\r\r버튼을 누르고 메서드 생성 을 선택합니다.  POST 메서드를 선택하고 v 를 누릅니다.  메서드에 포인트를 Lambda 함수 로 선택하고 Lambda 함수는 이전에 생성한 MyAurora 를 입력합니다.\r저장\r\r버튼을 누릅니다.  API의 Lambda 함수에 대한 권한 추가를 위해 확인\r\r버튼을 누릅니다.  \r작업\r\r버튼을 누르고, API 배포 를 선택합니다.  배포 스테이지는 새 스테이지를 선택하고 스테이지 이름을 PROD로 입력합니다. 배포\r\r버튼을 누릅니다.    REST API 테스트  API Gateway Console에 접속합니다. 생성한 API MyApi 를 선택합니다.  생성한 메소드 POST 를 선택합니다. 그리고 테스트 를 선택합니다.  요청 본문에 아래와 같이 입력하고 태스트\r\r버튼을 누릅니다.  테이블에 전체 데이터를 가지고 오는 요청    { \u0026#34;operation\u0026#34;: \u0026#34;get\u0026#34; } 요청 본문에 아래와 같이 입력하고 태스트\r\r버튼을 누릅니다.  role 이 admin 인 유저 데이터를 가지고 오는 요청    { \u0026#34;operation\u0026#34;: \u0026#34;para\u0026#34;, \u0026#34;payload\u0026#34;: { \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34; } } 실습1, 2가 모두 완료 되었습니다, 수고하셨습니다.\n\r"
},
{
	"uri": "/serverless/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/serverless/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/serverless/credits/",
	"title": "크레딧",
	"tags": [],
	"description": "",
	"content": "컨텐츠 최초제작 기여자  AWS CS Korea SA  sejun@    컨텐츠 업데이트 기여자  AWS CS Korea SA  sejun@ youngkko@ sehyul@    컨텐츠 웹 변환 기여자  AWS Korea SA  sejun@ inhwan@ sehyul@    소프트웨어 기여자 오픈 소스 소프트웨어를 더 좋게 만들어 주시는 기여자 분들께 감사드립니다!\n@vjeantet의 docdock, hugo-theme-learn의 분기에 대한 업적에 특별히 감사드립니다. 이 테마의 v2.0.0은 그의 작업으로 부터 영감 받았습니다.\n패키지와 라이브러리  Amazon Aurora - Amazon Aurora(Aurora)는 MySQL 및 PostgreSQL과 호환되는 완전 관리형 관계형 데이터베이스 엔진입니다. Tutorial: Using AWS Lambda with Amazon S3 - Suppose you want to create a thumbnail for each image file that is uploaded to a bucket. AWS SAM - The AWS Serverless Application Model (AWS SAM) is an open-source framework that you can use to build serverless applications  도구  Netlify - Continuous deployement and hosting of this documentation Hugo  "
}]